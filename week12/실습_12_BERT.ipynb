{"cells":[{"cell_type":"markdown","id":"lx5s0lWALq9_","metadata":{"id":"lx5s0lWALq9_"},"source":["실습을 시작하기 전에, 코드가 올바른 폴더에 위치해 있는지 점검해보자.\n","\n","**실습 파일과 데이터셋은 반드시 '내 드라이브' 위치에 있어야 한다.**\n","\n","뒤에 ratings.txt를 읽어오는데 어려움을 겪고 있다면, 파일들이 올바른 위치에 있는지 다시 한 번 점검해보자."]},{"cell_type":"markdown","id":"wNqoPN9nsdKt","metadata":{"id":"wNqoPN9nsdKt"},"source":["# **Preprocessing**\n","\n","우리는 문장을 입력해주었을 때, 그 문장이 긍정문인지 부정문인지 판단해주는 AI 모델을 구축하고자 한다. AI 모델이 문장을 올바르게 분류하기 위해서는 문장 내 정보들을 인지하는 과정이 필요하다.\n","\n","컴퓨터는 우리가 사용하는 언어를 있는 그대로 받아들일 수 없다. 컴퓨터가 이해할 수 있는 방식, 숫자로 변환해주어야 한다. 이와 같이 AI 모델이 주어진 데이터를 이해할 수 있도록, 데이터의 형태를 변환해주는 과정을 전처리 (Preprocessing) 라고 한다.\n","\n","**본 파일에서는 문장 데이터를 전처리하는 과정에 대해 다뤄볼 것이다.**\n","\n","전처리에 필요한 함수들을 구성한 뒤, 최하단의 preproc_test 함수를 통해서 테스트해볼 것이다."]},{"cell_type":"markdown","id":"_t5wa4UAuk96","metadata":{"id":"_t5wa4UAuk96"},"source":["실습을 시작하기에 앞서, 필요한 라이브러리들을 설치하자."]},{"cell_type":"code","execution_count":2,"id":"b7d3c81d","metadata":{"id":"b7d3c81d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722251305873,"user_tz":-540,"elapsed":10502,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"2a47fd51-dba9-4916-8045-066020d9ec2d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: mxnet in /usr/local/lib/python3.10/dist-packages (1.9.1)\n","Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.25.2)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.31.0)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from mxnet) (0.8.4)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.20.0->mxnet) (2024.7.4)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: transformers==4.20.1 in /usr/local/lib/python3.10/dist-packages (4.20.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (3.15.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (0.23.5)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (24.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (0.12.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.20.1) (4.66.4)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (2024.6.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.20.1) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.20.1) (2024.7.4)\n"]}],"source":["!pip install mxnet\n","!pip install sentencepiece\n","!pip install transformers==4.20.1\n","!pip install --upgrade -q pyproj"]},{"cell_type":"markdown","id":"ua7HcCVDvuMs","metadata":{"id":"ua7HcCVDvuMs"},"source":["아래의 mount 함수를 통해 구글 드라이브에 접근하자."]},{"cell_type":"code","execution_count":1,"id":"pdM0BfJEVz1k","metadata":{"id":"pdM0BfJEVz1k","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722251290757,"user_tz":-540,"elapsed":2915,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"a0b9eae8-1c99-45b0-fa43-f86d8feb0221"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["!cp /content/gdrive/MyDrive/OUTTA/week12/tokenization.py /content/"],"metadata":{"id":"JODK-_g3R2zB","executionInfo":{"status":"ok","timestamp":1722251334027,"user_tz":-540,"elapsed":2807,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"id":"JODK-_g3R2zB","execution_count":3,"outputs":[]},{"cell_type":"markdown","id":"23jcqTeGwzgk","metadata":{"id":"23jcqTeGwzgk"},"source":["연습 문제에 필요한 라이브러리들을 import하자."]},{"cell_type":"code","execution_count":4,"id":"74f1716b","metadata":{"id":"74f1716b","executionInfo":{"status":"ok","timestamp":1722251341020,"user_tz":-540,"elapsed":6420,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook\n","from sklearn.model_selection import train_test_split\n","from keras.utils import pad_sequences\n","import pandas as pd\n","import traceback\n","import torch\n","from transformers import get_linear_schedule_with_warmup, BertForSequenceClassification, BertConfig\n","import random\n","from tqdm.notebook import tqdm\n","import time\n","# KoBERTTokenizer를 불러오자.\n","from tokenization import KoBertTokenizer"]},{"cell_type":"markdown","id":"I2Bzrqxxxo7a","metadata":{"id":"I2Bzrqxxxo7a"},"source":["Pandas 라이브러리의 read_csv 함수를 활용하여 데이터셋을 읽어오자."]},{"cell_type":"code","execution_count":5,"id":"s58zYf3TdJiJ","metadata":{"id":"s58zYf3TdJiJ","executionInfo":{"status":"ok","timestamp":1722251342034,"user_tz":-540,"elapsed":1016,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["whole_dataset = pd.read_csv('/content/gdrive/MyDrive/OUTTA/week12/ratings.txt', delimiter=\"\\t\")"]},{"cell_type":"markdown","id":"BgRtfBYvxzLY","metadata":{"id":"BgRtfBYvxzLY"},"source":["읽어 온 데이터의 형태는 어떠할까?\n","\n","head 함수를 활용하여 상단 5개의 data들을 출력해보고, data의 row와 column은 어떻게 구성되어 있는지 살펴보자."]},{"cell_type":"code","execution_count":6,"id":"Y1bOaRtrZ1D9","metadata":{"id":"Y1bOaRtrZ1D9","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1722251342034,"user_tz":-540,"elapsed":10,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"649ba5a0-3792-43c1-abbc-0ec4cbd834c1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["         id                                           document  label\n","0   8112052                                어릴때보고 지금다시봐도 재밌어요ㅋㅋ      1\n","1   8132799  디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...      1\n","2   4655635               폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.      1\n","3   9251303  와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...      1\n","4  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1"],"text/html":["\n","  <div id=\"df-7ef189bc-0e94-4c3d-9881-9e019f3a1b60\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>document</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>8112052</td>\n","      <td>어릴때보고 지금다시봐도 재밌어요ㅋㅋ</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>8132799</td>\n","      <td>디자인을 배우는 학생으로, 외국디자이너와 그들이 일군 전통을 통해 발전해가는 문화산...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>4655635</td>\n","      <td>폴리스스토리 시리즈는 1부터 뉴까지 버릴께 하나도 없음.. 최고.</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>9251303</td>\n","      <td>와.. 연기가 진짜 개쩔구나.. 지루할거라고 생각했는데 몰입해서 봤다.. 그래 이런...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>10067386</td>\n","      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7ef189bc-0e94-4c3d-9881-9e019f3a1b60')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-7ef189bc-0e94-4c3d-9881-9e019f3a1b60 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-7ef189bc-0e94-4c3d-9881-9e019f3a1b60');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-bf36b0b3-aa7a-4ae5-b179-a06587ce7542\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bf36b0b3-aa7a-4ae5-b179-a06587ce7542')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-bf36b0b3-aa7a-4ae5-b179-a06587ce7542 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"whole_dataset"}},"metadata":{},"execution_count":6}],"source":["whole_dataset.head()"]},{"cell_type":"code","source":["whole_dataset['label']"],"metadata":{"id":"ic8itiVuUHYt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722251342035,"user_tz":-540,"elapsed":9,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"771195e2-9e74-48a4-b132-9accdf35d75f"},"id":"ic8itiVuUHYt","execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0         1\n","1         1\n","2         1\n","3         1\n","4         1\n","         ..\n","199995    0\n","199996    0\n","199997    0\n","199998    0\n","199999    0\n","Name: label, Length: 200000, dtype: int64"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["# Preprocess"],"metadata":{"id":"Rw_n-yzvyZA1"},"id":"Rw_n-yzvyZA1"},{"cell_type":"markdown","id":"EjMmau3TyPHo","metadata":{"id":"EjMmau3TyPHo"},"source":["데이터를 살펴보니, row는 0부터 차례로 이어지며, column은 id, document, label로 구성되어 있다.\n","\n","id는 데이터를 구분짓기 위한 일련 번호, document는 문장 데이터, label은 해당하는 문장이 긍정(1), 부정(0)인지 나타내는 라벨 값이다.\n","\n","여기서 우리는 학습 데이터들 간에 구분할 필요가 없으므로, id 데이터는 분리하자. 그리고 column 이름으로 document보다는 sentence가 적절해보이니, 그렇게 바꿔보자."]},{"cell_type":"code","execution_count":8,"id":"57f17468","metadata":{"id":"57f17468","executionInfo":{"status":"ok","timestamp":1722251342035,"user_tz":-540,"elapsed":8,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["def data_processing(raw_data):\n","    transformed_data = raw_data['label']\n","\n","    # 판다스의 concat을 활용하여 'document' 데이터와 'label' 데이터를 연결해보자.\n","    # [1] new_array = pandas.concat([array1, array2])를 하면, array1과 array2가 물리적으로 연결된 new_array를 생성할 수 있다.\n","    #     이때 추가 옵션을 별도로 지정하지 않았으므로, concat을 수행하는 default 방향은 axis=0(default)이다.\n","    # [2] concat 시 axis 조건을 통해 array를 concat하는 방향을 직접 지정해주도록 하자.\n","    #     Hint: 2차원 array에서 axis=0은 세로(상/하) 방향, axis=1은 가로(좌/우) 방향이다.\n","    #     우리는 'document'열과 'label' 열을 가로(좌/우) 방향으로 concat해야 하므로 concat 시 axis=1이라는 조건을 설정하는 것이 적합할 것이다.\n","    # concat에 대한 추가적인 내용은 https://yganalyst.github.io/data_handling/Pd_12/ 를 참고해보자.\n","    # axis에 대한 추가적인 내용은 https://jalammar.github.io/visual-numpy/ https://lets-hci-la-ai-withme.tistory.com/15 를 참고해보자.\n","    ## 여기에 코드 작성\n","    processed_data = pd.concat([raw_data['document'], transformed_data], axis=1)\n","\n","\n","    # 데이터의 column 이름을 sentence label로 바꾸어준다.\n","    processed_data.columns = ['sentence', 'label']\n","\n","    return processed_data"]},{"cell_type":"markdown","id":"wKH9R1WuEnck","metadata":{"id":"wKH9R1WuEnck"},"source":["여기까지 구현한 부분을 하단의 preproc_test 함수의 첫 번째 테스트로 확인해볼 것이다. (20점)"]},{"cell_type":"markdown","id":"ZNO2Hi2Q4dpb","metadata":{"id":"ZNO2Hi2Q4dpb"},"source":["이제 문장 데이터를 본격적으로 변환시켜볼 것이다.\n","\n","아래 data_to_token_ids 함수를 아래와 같은 절차를 통해 문장 데이터를 수치적으로 변환한다.\n","\n","1. Tokenizer가 문장의 시작과 끝을 인식할 수 있도록 문장 앞뒤에 CLS 토큰과 SEP 토큰을 붙인다.\n","\n","2. Tokenizer의 tokenize 함수를 활용하여 문장을 여러 개의 토큰으로 나눈다.\n","\n","3. Tokenizer의 convert_tokens_to_ids 함수를 활용하여, 토큰들을 대응되는 id로 변환해준다.\n","\n","4. MAX_LEN의 길이에 맞춰, padding을 진행해준다. 비어있는 자리의 경우, 0이 입력된다."]},{"cell_type":"code","execution_count":9,"id":"71055cfe","metadata":{"id":"71055cfe","executionInfo":{"status":"ok","timestamp":1722251342035,"user_tz":-540,"elapsed":7,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["def data_to_token_ids(tokenizer, single_sentence):\n","    # CLS 토큰과 SEP 토큰을 문장의 시작과 끝에 붙여보자.\n","    # CLS 토큰은 '[CLS]', SEP 토큰은 '[SEP]' 로 작성하자.\n","    # hint : 'special_token_added'는 반드시 'string' 타입, 즉 문자열 타입이어야 한다!!\n","    ## 여기에 코드 작성\n","    special_token_added = \"[CLS]\" + str(single_sentence) +\"[SEP]\"\n","    # tokenizer의 tokenize 함수를 활용하여 문장을 토큰화해보자.\n","    tokenized_sentence = tokenizer.tokenize(special_token_added)\n","\n","    # tokenizer의 convert_tokens_to_ids 함수를 활용하여 생성된 토큰을 숫자 형태로 바꿔주자.\n","    token_ids = [tokenizer.convert_tokens_to_ids(tokenized_sentence)]\n","\n","    MAX_LEN = 128\n","    # pad_sequences 함수를 활용하여 문장의 빈 칸에 padding을 넣어주자.\n","    # keras의 preprocessing.sequence 라이브러리는 pad_sequences 함수를 제공하며, 본 함수는 서로 다른 길이의 문장을 특정 길이(최대 길이)로 일치시키기 위해 truncating와 padding을 한다.\n","    # truncating은 최대 길이보다 긴 문장을 최대 길이에 맞게 잘라내는 것이고, padding은 최대 길이보다 짧은 문장의 남는 자리를 0으로 채우는 것을 가리킨다.\n","    # pad_sequence는 default 옵션은 'pre'이다. 즉 긴 문장의 앞 부분을 잘라내거나 짧은 문장의 앞에 0을 채운다.\n","    # 그러나 우리는 토큰 id 리스트의 '뒷'부분에 truncating 및 padding을 적용해주고자 한다.\n","    # truncating=\"post\" 옵션을 통해 id 리스트의 길이가 MAX_LEN을 넘어가는 부분에 대해서는 뒷부분을 삭제할 수 있다.\n","    # padding=\"post\" 옵션을 통해 토큰 id 리스트의 뒷부분에 padding을 적용할 수 있다.\n","    # 최대 길이는 위의 MAX_LEN으로 설정하고, dtype은 long으로 설정해주자.\n","    # 위 설정에 맞게 padding, truncating 값을 적절히 설정해주자.\n","    # https://www.tensorflow.org/api_docs/python/tf/keras/utils/pad_sequences\n","    ## 여기에 코드 작성\n","    token_ids_padded = pad_sequences(token_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","\n","    token_ids_flatten = token_ids_padded.flatten()\n","    return token_ids_flatten"]},{"cell_type":"markdown","id":"f_37Yb-NqxgA","metadata":{"id":"f_37Yb-NqxgA"},"source":["완성한 data_to_token_ids 함수를 간단히 활용해보며 함수의 반환값 형태를 익혀보자.  \n","문장이 토큰화되고 각 토큰이 id값으로 반환되었으며, 최대 길이에 미치지 못하는 부분은 문장의 뒷부분에 0으로 padding 처리가 되었음을 확인할 수 있다.  \n","\n","\n","\n","```\n","from tokenization import KoBertTokenizer\n","tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n","\n","id_testing = data_to_token_ids(tokenizer, \"찐배고픔이랑 가짜배고픔이랑 구분하는건 이미포기했어\")\n","\n","print(id_testing)\n","\n","# [   2  517 7385 6312 5439 7766 7096 6022  770 6312 5439 7766 7096 6022\n"," 1115 6416 7794 5384 3692 7728 5561 7864 6855    3    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n","    0    0]\n","\n","```\n","\n","위와 같은 방식으로 tokenizer와 임의의 한국어 문장을 입력하여, 함수가 정확히 만들어졌는지 확인해보도록 하자.\n"]},{"cell_type":"markdown","id":"h9yYk0CI87Td","metadata":{"id":"h9yYk0CI87Td"},"source":["pad_sequences 함수를 통해 padding이 이루어진 부분은 학습하는데 실질적으로 쓰이지 않는다.\n","\n","그러므로, padding된 부분은 고려하지 않도록 필터링해주는 mask를 만들어보자.\n","\n","여기서 mask는 padding된 부분은 지우고, 나머지 부분은 그대로 두는 기능을 한다. 고로, padding된 부분은 0, 아닌 부분은 1의 값을 나타내도록 한다."]},{"cell_type":"code","execution_count":10,"id":"0ec337a5","metadata":{"id":"0ec337a5","executionInfo":{"status":"ok","timestamp":1722251342035,"user_tz":-540,"elapsed":7,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["def token_ids_to_mask(token_ids):\n","\n","    # 한 문장에 대한 token_id 리스트를 입력으로 받는다.\n","    # token_id에서 0보다 큰 숫자만 유효하도록 하는 'mask' 리스트를 만들자.\n","    # 이 때, mask의 각 원소는 0 아니면 1의 값을 가져야 한다.\n","    # Hint : 각 token_id를 0 아니면 1의 값으로 바꿔주면 된다.\n","    # Hint : list comprehension을 활용해서 작성하면 편하다.\n","    # HINT : list comprehension을 활용하는 것이 가장 간결한 코드이며, 조금 어렵다면 for 문을 활용해봐도 좋다.\n","    # list comprehension 참고링크 :: https://www.w3schools.com/python/python_lists_comprehension.asp\n","    ## 여기에 코드 작성\n","    mask = [1 if token_id > 0 else 0 for token_id in token_ids]\n","\n","    return mask"]},{"cell_type":"markdown","id":"V-bizxlEq6ab","metadata":{"id":"V-bizxlEq6ab"},"source":["마찬가지로, 완성한 token_ids_to_mask 함수를 간단히 활용해보며 함수의 반환값 형태를 익혀보자.  \n","0으로 패딩처리된 부분은 0.0으로, 나머지는 1.0으로 채워진 max_length 길이의 array가 반환됨을 확인할 수 있다.\n","\n","```\n","mask_testing = token_ids_to_mask(id_testing)\n","print(mask_testing)\n","\n","# [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","```\n","\n","위와 같은 방식으로, 앞서 생성한 id_testing을 입력하여 함수가 정확히 만들어졌는지 확인해보도록 하자.\n"]},{"cell_type":"markdown","id":"EsMqQ9qI-Mad","metadata":{"id":"EsMqQ9qI-Mad"},"source":["data_to_token_ids 함수는 하나의 문장이 입력되었을 때, token_id 리스트를 출력해주는 함수이며, token_ids_to_mask 함수는 한 문장에 대한 token_id 리스트가 입력되었을 때, 하나의 mask를 생성해주는 함수다.\n","위에서 구성한 두 가지 함수를 활용해서, 전체 데이터를 변형하는 로직을 구성해보자.\n","\n","*HINT : list comprehension을 활용하는 것이 가장 간결한 코드이며, 조금 어렵다면 for 문을 활용해봐도 좋다.*"]},{"cell_type":"code","execution_count":11,"id":"c0509ac6","metadata":{"id":"c0509ac6","executionInfo":{"status":"ok","timestamp":1722251342035,"user_tz":-540,"elapsed":7,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["# tokenize_processed_data 함수는, 앞서 정의한 함수인 data_to_token_ids와 token_ids_to_mask를 모두 활용한다.\n","# tokenize_processed_data 함수를 통해, 데이터를 구성하는 각 문장을 '토큰 id로 구성된, max_length 길이의 array'로 변환해주고, attention 마스크를 생성하는 작업을 한꺼번에 처리할 수 있다.\n","# 함수의 입력은 두 가지로, [1]tokenizer, [2] raw_data를 data_processing 함수로 전처리한 결과인 processed_dataset이다.\n","# 함수의 출력은 세 가지로, [1]tokenized_data (='토큰 id로 구성된 max_length 길이의 array'로 구성된 list), [2] 데이터의 labels, [3] 각 array에 대응하는 attention_masks로 구성된 list이다.\n","\n","def tokenize_processed_data(tokenizer, processed_dataset):\n","    labels = processed_dataset['label'].to_numpy()\n","\n","    # list comprehension을 활용하여 processed_dataset의 'sentence' 데이터를 id 리스트로 토큰화하자.\n","    # HINT : list comprehension을 활용하는 것이 가장 간결한 코드이며, 조금 어렵다면 for 문을 활용해봐도 좋다.\n","    # list comprehension 참고링크 :: https://www.w3schools.com/python/python_lists_comprehension.asp\n","    ## 여기에 코드 작성\n","    tokenized_data = [data_to_token_ids(tokenizer, sentence) for sentence in processed_dataset['sentence']]\n","\n","    # list comprehension을 활용하여 앞서 토큰화한 id 리스트 각각을 mask로 변환하자.\n","    ## 여기에 코드 작성\n","    attention_masks = [token_ids_to_mask(token_ids) for token_ids in tokenized_data]\n","\n","    return tokenized_data, labels, attention_masks"]},{"cell_type":"markdown","id":"2QPPr-B7EBFI","metadata":{"id":"2QPPr-B7EBFI"},"source":["여기까지 구현한 부분을 하단의 preproc_test 함수의 두 번째 테스트로 확인해볼 것이다. (40점)"]},{"cell_type":"markdown","id":"50qJbROTAD2n","metadata":{"id":"50qJbROTAD2n"},"source":["위의 함수를 통해 변환한 데이터셋을 train, validation, test용으로 나눠야 한다.\n","\n","split_into_train_test 함수를 통해 train 데이터와 test 데이터로 나누고, 그렇게 나누어진 train 데이터를 split_into_train_validation 함수를 통해 train 데이터와 validation 데이터로 나눌 것이다.\n","\n","sklearn.model_selection 라이브러리의 train_test_split 함수를 활용하여 아래의 두 함수를 구현해보자."]},{"cell_type":"code","execution_count":12,"id":"HW7i_vdmSXIn","metadata":{"id":"HW7i_vdmSXIn","executionInfo":{"status":"ok","timestamp":1722251342035,"user_tz":-540,"elapsed":7,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["def split_into_train_test(whole_data, whole_label, whole_masks):\n","    print(\"length of whole_data : \" + str(len(whole_data)))\n","\n","    train_inputs, test_inputs, train_labels, test_labels = train_test_split(whole_data,\n","                                                                                    whole_label,\n","                                                                                    random_state=2022,\n","                                                                                    test_size=0.1)\n","    # 위의 방식을 참조하여 mask 역시 train을 위한 mask와 test을 위한 mask로 나누자.\n","    # 이때 return 값을 참조하여, 우리에게 불필요한 정보는 _로 비워두자.\n","    # random_state와 test_size 동일하게 설정\n","    # train_test_split 함수 사용법 참고 링크 : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n","    ## 여기에 코드 작성\n","    train_masks, test_masks= train_test_split(whole_masks,\n","                                                     random_state=2022,\n","                                                     test_size=0.1)\n","\n","    return train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks"]},{"cell_type":"code","execution_count":13,"id":"t2ymZef0SrK-","metadata":{"id":"t2ymZef0SrK-","executionInfo":{"status":"ok","timestamp":1722251342035,"user_tz":-540,"elapsed":7,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["def split_into_train_validation(train_data, train_label, train_masks):\n","    print(\"length of train_data : \" + str(len(train_data)))\n","\n","    # split_into_train_test의 코드를 참조하여 data와 mask 각각을 train을 위한 것과 validation을 위한 것으로 나누자.\n","    # random_state = 2022, test_size = 0.1로 설정\n","    # train_test_split 함수 사용법 참고 링크 : https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n","\n","    ## 여기에 코드 작성(data)\n","    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_data,\n","                                                                                        train_label,\n","                                                                                        random_state=2022,\n","                                                                                        test_size=0.1)\n","\n","    ## 여기에 코드 작성(mask)\n","    train_masks, validation_masks= train_test_split(train_masks,\n","                                                            random_state=2022,\n","                                                            test_size=0.1)\n","\n","    return train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks"]},{"cell_type":"markdown","id":"MebtEGjsERzf","metadata":{"id":"MebtEGjsERzf"},"source":["여기까지 구현한 부분을 하단의 preproc_test 함수의 세 번째 테스트로 확인해볼\n","것이다. (60점)"]},{"cell_type":"markdown","id":"uKrLg_DKBJ-6","metadata":{"id":"uKrLg_DKBJ-6"},"source":["우리는 이렇게 나누어진 데이터들을 tensor의 형태로 변환해주어야 한다. data_to_tensor는 그러한 역할을 해주는 함수다.\n","\n","torch.tensor 함수를 활용해서 inputs, labels, masks 각각을 tensor로 변환해주는 함수를 구현해보자."]},{"cell_type":"code","execution_count":14,"id":"txWqnnYCT9dT","metadata":{"id":"txWqnnYCT9dT","executionInfo":{"status":"ok","timestamp":1722251342035,"user_tz":-540,"elapsed":6,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["def data_to_tensor(inputs, labels, masks):\n","    # 입력받은 데이터를 텐서로 변환해주는 함수\n","    # torch.tensor() 사용하기\n","    ## 여기에 코드 작성\n","    inputs_tensor = torch.tensor(inputs)\n","    labels_tensor = torch.tensor(labels)\n","    masks_tensor = torch.tensor(masks)\n","    return inputs_tensor, labels_tensor, masks_tensor"]},{"cell_type":"markdown","id":"iAwpViRKETw3","metadata":{"id":"iAwpViRKETw3"},"source":["여기까지 구현한 부분을 하단의 preproc_test 함수의 네 번째 테스트로 확인해볼 것이다. (80점)"]},{"cell_type":"markdown","id":"r0HUfeHmBUDE","metadata":{"id":"r0HUfeHmBUDE"},"source":["tensor로 변환한 데이터를 dataloader를 활용해서 batch 단위로 묶어줄 것이다.\n","\n","batch로 데이터를 묶어주기 전에, 데이터를 어떤 순서로 뽑을 것인지 적절한 Sampler를 설정해주어야 한다. 데이터가 학습을 위한 것인지, 검증을 위한 것인지에 따라서 알맞은 Sampler를 배정해주도록 하자."]},{"cell_type":"code","execution_count":15,"id":"CYvW5kkHUab-","metadata":{"id":"CYvW5kkHUab-","executionInfo":{"status":"ok","timestamp":1722251342035,"user_tz":-540,"elapsed":6,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["# 모든 DataLoader는 Sampler를 갖고 있다. Sampler는 데이터를 load 해올 때 데이터의 index를 컨트롤함으로써 어떤 데이터부터 가져올지 지정하는 기능을 한다.\n","# - SequentialSampler: 항상 같은 순서로, 순차적으로 데이터를 load 한다.\n","# - RandomSampler: 랜덤하게 데이터를 load 한다.\n","\n","def tensor_to_dataloader(inputs, labels, masks, mode):\n","    from torch.utils.data import RandomSampler, SequentialSampler\n","\n","    batch_size=32\n","    data = TensorDataset(inputs, masks, labels)\n","\n","    if mode == \"train\":\n","        # train 모드에서는 랜덤하게 데이터를 load해오는 sampler를 사용하자.\n","        # 대개 mini-batch 내부 구성이 다양할수록 전체 dataset(모집단)를 잘 대표하기 때문에 주로 RandomSampler를 사용한다.\n","        # 참고 링크 : https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler\n","        ## 여기에 코드 작성\n","        sampler = RandomSampler(data)\n","    else:\n","        # test에는 순차적으로 데이터를 load하는 sampler을 지정하자.\n","        # 참고 링크 : https://pytorch.org/docs/stable/data.html#torch.utils.data.SequentialSampler\n","        ## 여기에 코드 작성\n","        sampler = SequentialSampler(data)\n","\n","    # DataLoader 함수를 활용해서 dataloader를 선언해보자.\n","    # batch_size는 batch_size로 설정하고, sampler는 위에서 지정한대로 설정해주자.\n","    # DataLoader 사용법 : https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n","    # sampler를 꼭 설정할것!\n","    ## 여기에 코드 작성\n","    dataloader = DataLoader(data, batch_size=batch_size, sampler=sampler)\n","\n","    return dataloader"]},{"cell_type":"markdown","id":"OgafT913EYuI","metadata":{"id":"OgafT913EYuI"},"source":["여기까지 구현한 부분을 하단의 preproc_test 함수의 다섯 번째 테스트로 확인해볼 것이다. (100점)"]},{"cell_type":"markdown","id":"W75rhx9mBjT3","metadata":{"id":"W75rhx9mBjT3"},"source":["이제 전처리를 수행하는데 필요한 모든 함수들을 다 구현했다.\n","\n","구현한 함수들을 모아 preproc 함수를 최종적으로 만들었다."]},{"cell_type":"code","execution_count":16,"id":"R0BRFQIyPd2n","metadata":{"id":"R0BRFQIyPd2n","executionInfo":{"status":"ok","timestamp":1722251342036,"user_tz":-540,"elapsed":7,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["def preproc(tokenizer, whole_dataset):\n","    # 지금까지 만든 함수들을 이 함수 안에서 사용해서 구현하면 된다. (라이브러리를 사용하는 것이 아님)\n","    # whole_dataset을 전처리하자.\n","    processed_dataset = data_processing(whole_dataset)\n","\n","    # 전처리한 전체 데이터를 토큰화하자.\n","    tokenized_dataset, labels, attention_masks = tokenize_processed_data(tokenizer, processed_dataset)\n","\n","    # 토큰화한 전체 데이터를 train용과 test용으로 분리하자.\n","    train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks = split_into_train_test(tokenized_dataset, labels, attention_masks)\n","    # 토큰화한 train용 데이터를 train용과 validation용으로 분리하자.\n","    train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = split_into_train_validation(train_inputs, train_labels, train_masks)\n","    # train용, validation용, test용 데이터 각각을 텐서로 변환하자.\n","    train_inputs, train_labels, train_masks = data_to_tensor(train_inputs, train_labels, train_masks)\n","    validation_inputs, validation_labels, validation_masks = data_to_tensor(validation_inputs, validation_labels, validation_masks)\n","    test_inputs, test_labels, test_masks = data_to_tensor(test_inputs, test_labels, test_masks)\n","\n","    # train용, validation용, test용 텐서를 dataloader로 변환하자.\n","    train_dataloader = tensor_to_dataloader(train_inputs, train_labels, train_masks, \"train\")\n","    validation_dataloader = tensor_to_dataloader(validation_inputs, validation_labels, validation_masks, \"validation\")\n","    test_dataloader = tensor_to_dataloader(test_inputs, test_labels, test_masks, \"test\")\n","\n","    return train_dataloader, validation_dataloader, test_dataloader"]},{"cell_type":"markdown","id":"Z0d0FcgnDUm-","metadata":{"id":"Z0d0FcgnDUm-"},"source":["함수들이 잘 적절히 잘 만들어졌는지 preproc_test 함수를 통해 측정해보자.\n","\n","5개의 테스트로 구성되어 있으며, 각 테스트는 20점이다.\n","\n","첫 번째 테스트는 별도의 함수는 존재하지 않는다."]},{"cell_type":"code","source":["def test2(tokenized_data):\n","    real_data = [2, 1706, 6664, 5729, 6983,  517, 7990, 6493, 7828, 5943, 4928, 1861, 5783, 2235,\n","                 6527,   54, 7227, 6160, 3010, 6559, 7828, 2846, 7095, 3394, 6946,   54, 5782, 6150,\n","                 3093, 6653, 7010, 5384, 3647, 2846, 6116, 4147, 6441,  517, 5693, 5693, 7828, 4768,\n","                 5330,  743, 5451, 6903, 4147, 7869, 6198, 4102, 2034, 7170, 7792, 4709, 7879, 7328,\n","                 54, 1185, 6049, 5782, 5439, 5007, 3647, 2680, 5330, 3135, 7271, 5782, 5760, 5384,\n","                 1861, 3100,   54, 1569, 4196, 3093, 6653, 7013, 2571,   54,    3,    0,    0,    0,\n","                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0]\n","    return (tokenized_data[2022] == real_data).all()\n","\n","\n","def test3(train_inputs, validation_inputs, test_masks):\n","    if len(train_inputs) != 162000 or len(validation_inputs) != 18000 or len(test_masks) != 20000:\n","        return False\n","    real_data = [2, 3765, 6954, 4207, 7850, 4446, 6395, 5761, 4102, 3977, 6881, 6701,   54, 2368,\n","                 517, 7265, 6827, 6701,   54,    3,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                 0,    0]\n","    return (train_inputs[2022] == real_data).all()\n","\n","\n","def test4(train_inputs):\n","    real_input = torch.tensor([2, 3301, 6553, 6410,  517, 6193, 7591, 4179, 6141, 6255, 4244, 5439,\n","                               4012,  517, 6193, 7591, 1370, 5347, 5782, 5330, 2573, 6844, 7495, 1844,\n","                               6190, 1734, 6978, 7968, 7720, 7086,  517, 6193, 7591, 4179, 7788,  517,\n","                               6394, 5833, 6141, 7318, 6149, 7086, 3524, 7227, 5859, 7136, 5546, 5850,\n","                               2034, 7170, 7095, 1369, 5760, 1420,   55,    3,    0,    0,    0,    0,\n","                               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                               0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                               0,    0,    0,    0,    0,    0,    0,    0])\n","\n","    return torch.equal(train_inputs[12345], real_input)\n","\n","\n","def test5(train_dataloader):\n","    real_input_ids = torch.tensor([2,  529,   54, 2860, 6295, 7640, 5371, 3594, 7837,  553,   54,  773,\n","                                   6383, 7095, 5037, 6645, 7837, 4501, 5957, 6629, 7288, 3714, 7207, 5357,\n","                                   589,   54, 2417, 5398, 6882, 3357,  631,  529, 7220,    3,    0,    0,\n","                                   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                                   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                                   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                                   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                                   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                                   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                                   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n","                                   0,    0,    0,    0,    0,    0,    0,    0])\n","\n","    for step, batch in enumerate(tqdm(train_dataloader)):\n","\n","        if step < 1234:\n","            continue\n","        if step > 1234:\n","            break\n","\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","    return torch.equal(b_input_ids[5], real_input_ids)\n","\n","def preproc_test(tokenizer, whole_dataset):\n","\n","    print(\"================={}번째 테스트 시작===================\".format(1))\n","    # whole_dataset을 전처리하자.\n","    try:\n","      processed_dataset = data_processing(whole_dataset)\n","    except:\n","      print(traceback.format_exc())\n","      return 0\n","    print(\"================={}번째 테스트 성공===================\\n\".format(1))\n","\n","\n","    print(\"================={}번째 테스트 시작===================\".format(2))\n","    # 전처리한 전체 데이터를 토큰화하자.\n","    try:\n","      tokenized_dataset, labels, attention_masks = tokenize_processed_data(tokenizer, processed_dataset)\n","    except:\n","      print(traceback.format_exc())\n","      return 20\n","    if not test2(tokenized_dataset):\n","      return 20\n","    print(\"================={}번째 테스트 성공===================\\n\".format(2))\n","\n","\n","    print(\"================={}번째 테스트 시작===================\".format(3))\n","    # 토큰화한 전체 데이터를 train용과 test용으로 분리하자.\n","    try:\n","      train_inputs, test_inputs, train_labels, test_labels, train_masks, test_masks = split_into_train_test(tokenized_dataset, labels, attention_masks)\n","      # 토큰화한 train용 데이터를 train용과 validation용으로 분리하자.\n","      train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = split_into_train_validation(train_inputs, train_labels, train_masks)\n","    except:\n","      print(traceback.format_exc())\n","      return 40\n","    if not test3(train_inputs, validation_inputs, test_masks):\n","      return 40\n","    print(\"================={}번째 테스트 성공===================\\n\".format(3))\n","\n","\n","    print(\"================={}번째 테스트 시작===================\".format(4))\n","    # train용, validation용, test용 데이터 각각을 텐서로 변환하자.\n","    try:\n","      train_inputs, train_labels, train_masks = data_to_tensor(train_inputs, train_labels, train_masks)\n","      validation_inputs, validation_labels, validation_masks = data_to_tensor(validation_inputs, validation_labels, validation_masks)\n","      test_inputs, test_labels, test_masks = data_to_tensor(test_inputs, test_labels, test_masks)\n","    except:\n","      print(traceback.format_exc())\n","      return 60\n","    if not test4(train_inputs):\n","      return 60\n","    print(\"================={}번째 테스트 성공===================\\n\".format(4))\n","\n","\n","    print(\"================={}번째 테스트 시작===================\".format(5))\n","    # train용, validation용, test용 텐서를 dataloader로 변환하자.\n","    try:\n","      train_dataloader = tensor_to_dataloader(train_inputs, train_labels, train_masks, \"train\")\n","      validation_dataloader = tensor_to_dataloader(validation_inputs, validation_labels, validation_masks, \"validation\")\n","      test_dataloader = tensor_to_dataloader(test_inputs, test_labels, test_masks, \"test\")\n","    except:\n","      print(traceback.format_exc())\n","      return 80\n","    if not test5(train_dataloader):\n","      return 80\n","    print(\"================={}번째 테스트 성공===================\\n\".format(5))\n","\n","\n","    return 100"],"metadata":{"id":"-dteCynSUcC8","executionInfo":{"status":"ok","timestamp":1722251342486,"user_tz":-540,"elapsed":457,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"id":"-dteCynSUcC8","execution_count":17,"outputs":[]},{"cell_type":"code","execution_count":18,"id":"7U6SEQwTcZrE","metadata":{"id":"7U6SEQwTcZrE","colab":{"base_uri":"https://localhost:8080/","height":488,"referenced_widgets":["291cb03fe68e434db50c38fdd02b4502","79d8e815bf7b473b8be7513ce9d05d5f","8c1437d2743046398113a9c528c69b8f","f608397c0de14b7aa0995e62b4526b0f","2c3fc077c0074895b118ec38d2874c22","779d20f9061441cf98b59684043fdaf1","b89b7b825c88471085e6630c04c93bac","1fe3685e8f7a4d5fa83f8f1b4c6685f8","3bf4fcf4f0884df7990f09f38c8b8d13","303ed69f45534fa385109d30b6797334","c41b2fd5e14e4356ab4217c722d6aabc"]},"executionInfo":{"status":"ok","timestamp":1722251396526,"user_tz":-540,"elapsed":54041,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"c21d0622-e0e9-408e-ac48-7b4314d4d50a"},"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'KoBertTokenizer'.\n"]},{"output_type":"stream","name":"stdout","text":["=================1번째 테스트 시작===================\n","=================1번째 테스트 성공===================\n","\n","=================2번째 테스트 시작===================\n","=================2번째 테스트 성공===================\n","\n","=================3번째 테스트 시작===================\n","length of whole_data : 200000\n","length of train_data : 180000\n","=================3번째 테스트 성공===================\n","\n","=================4번째 테스트 시작===================\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-14-389286a030fd>:5: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n","  inputs_tensor = torch.tensor(inputs)\n"]},{"output_type":"stream","name":"stdout","text":["=================4번째 테스트 성공===================\n","\n","=================5번째 테스트 시작===================\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5063 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"291cb03fe68e434db50c38fdd02b4502"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["=================5번째 테스트 성공===================\n","\n","현재 점수 : 100/100점\n"]}],"source":["# 시드 고정\n","seed_val = 2022\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)\n","\n","\n","tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n","score = preproc_test(tokenizer, whole_dataset)\n","print(\"현재 점수 : {}/100점\".format(score))"]},{"cell_type":"markdown","id":"0k1-rDdeIxJf","metadata":{"id":"0k1-rDdeIxJf"},"source":["아래 쉘을 실행하면 테스트의 점수를 알 수 있다.\n","\n","***100점이 되어야 다음 섹터로 넘어갈 수 있다.***"]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"XEnOk0azyVMj"},"id":"XEnOk0azyVMj"},{"cell_type":"markdown","source":["이번 실습의 task는 주어진 입력 문장을 긍정/부정으로 이진분류하는 모델을 만드는 것이다. 이진분류를 위한 모델을 생성하기 위해서, 우리는 사전 훈련된 BERT 모델을 가지고 온 뒤 네이버 영화 리뷰 데이터셋을 활용하여 해당 모델을 긍정/부정의 이진분류를 수행하는 모델로 학습시켜야 한다.\n","\n","**본 실습에서는 사전 훈련된 BERT 모델을 가지고 와서 우리의 task에 필요한 모델을 생성하고, 모델의 학습 과정에 필요한 하이퍼 파라미터를 설정하는 과정을 다뤄볼 것이다.**"],"metadata":{"id":"5uf1_AODyzE9"},"id":"5uf1_AODyzE9"},{"cell_type":"markdown","source":["감성 이진 분류 task를 위한 BertModel을 생성하자.\n","- 우선, 우리가 생성할 모델을 저장할 PATH를 지정하자.\n","- 이후, 본 과제의 task가 무엇인지 고려하여 HuggingFace 홈페이지에서 우리에게 필요한 BertModel 형식이 무엇인지 파악하자. 그리고 해당 모델을 이용하여 model을 생성하자. (*본 과제가, 영화리뷰를 긍정과 부정의 두 가지 감정으로 분류해내는 작업임을 고려해보자.)\n","- 참고로 우리의 모델은 monologg의 kobert을 사전 훈련된 모델로 사용할 것이다. 사전 훈련 모델을 사용하기 위해서 from_pretrained 함수를 사용해야 하며, 함수의 파라미터에 'monologg/kobert'와 라벨 개수(num_labels)를 지정해야 한다.\n","- 모델을 생성하였다면, 미리 지정해둔 PATH에 해당 모델을 저장한다."],"metadata":{"id":"8lW77-VTzDkJ"},"id":"8lW77-VTzDkJ"},{"cell_type":"code","execution_count":19,"id":"AOloXaFbXo-X","metadata":{"id":"AOloXaFbXo-X","executionInfo":{"status":"ok","timestamp":1722251396526,"user_tz":-540,"elapsed":5,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["# [분류를 위한 BERT 모델 생성: BertModel을 초기화하는 역할]\n","def BertModelInitialization():\n","    PATH = \"/content/gdrive/MyDrive/model.pt\"\n","\n","    # BertModel은 다양한 작업을 진행할 수 있도록 여러 인터페이스들을 제공한다.\n","    # 본 중간 미션의 task가 '영화리뷰(Sequence)를 긍정과 부정의 두 가지 감정으로 분류하기(Classification)'이다.\n","    # Bert에서 시퀀스를 분류하는 인터페이스로 BertForSequenceClassification가 제공되고 있다.\n","    # 이 외 제공되는 인터페이스 종류가 궁금하다면 HuggingFace 홈페이지에 Bert를 검색해서 찾아보자: https://huggingface.co/docs/transformers/main/en/index\n","    # 추가로, monologg에 의해 사전훈련된 kobert 모델을 가지고 와야 하므로 .from_pretrained('monologg/kobert')를 쓰고, 분류 라벨 수가 2개 이므로 'num_labels=2'를 추가 입력한다.\n","\n","    model = BertForSequenceClassification.from_pretrained('monologg/kobert', num_labels=2)\n","\n","\n","    torch.save(model.state_dict(), PATH) # 생성한 모델을 특정 PATH에 저장하기"]},{"cell_type":"markdown","source":["get_model은 생성한 BertModel을 불러와서, 그것을 우리의 디바이스에 등록하는 함수이다."],"metadata":{"id":"GvVoeGKUzNmq"},"id":"GvVoeGKUzNmq"},{"cell_type":"code","execution_count":20,"id":"5-OfuvBSaR_N","metadata":{"id":"5-OfuvBSaR_N","executionInfo":{"status":"ok","timestamp":1722251396526,"user_tz":-540,"elapsed":4,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"outputs":[],"source":["def get_model(device):\n","    PATH = \"/content/gdrive/MyDrive/model.pt\"\n","\n","    model = BertForSequenceClassification.from_pretrained('monologg/kobert')\n","\n","    model.load_state_dict(torch.load(PATH)) # PATH에 저장된 모델을 불러오기\n","    model = model.to(device) # 불러온 모델을 device에 올리기\n","\n","    return model"]},{"cell_type":"markdown","source":["이번에는, model의 학습 초매개변수(옵티마이저, 에포크, 훈련 스텝, 스케줄러)를 설정하는 **get_model_with_params** 함수를 정의하자.\n","- 우리는 학습 단계에서 최적의 매개변수를 찾아내기 위하여 옵티마이저를 사용할 수 있다. 대표적인 옵티마이저로 AdamW가 있고, 이 외에도 이하와 같이 다양한 옵티마이저들이 있다.\n","\n","> Batch Gradient Descent.\n","Stochastic Gradient Descent.\n","Momentum. Nesterov Accelerated Gradient\n","(NAG)\n","Adagrad.\n","RMSprop.\n","Adam.\n","\n","- 러닝 스케줄러는 학습이 이루어짐에 따라 learning_rate을 감소시키는 도구이다. 러닝 스케줄러 역시 get_linear_schedule_with_warmup 외에 다양한 종류가 있다.\n","\n","우선 중간미션에서는 AdamW으로 옵티마이저 종류 및 파라미터 내용, 학습 에폭 수, 총 훈련 스텝, 러닝 스케줄러를 모두 고정할 것이다. 최종 미션에서는 이러한 초매개변수를 직접 조절하면서 모델 성능 향상을 위한 다양한 방법을 시도해볼 것이다."],"metadata":{"id":"8tGMdamUzPXD"},"id":"8tGMdamUzPXD"},{"cell_type":"code","source":["def get_model_with_params(num_data, device):\n","    model = get_model(device)\n","\n","    # 옵티마이저 설정하기. lr = 1e-5, eps = 1e-8으로 설정할것!\n","    # 옵티마이저는 AdamW를 사용할 것!\n","    # AdamW 사용 방법 : https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5, eps=1e-8)\n","\n","    # 전체 데이터가 총 몇 번 학습되는지\n","    epochs = 1\n","\n","    # 총 훈련 스텝\n","    total_steps = num_data * epochs\n","\n","    # 학습이 이루어짐에 따라 learning_rate을 감소시키기 위한 스케줄러. num_training_steps를 total_steps으로 설정할것.\n","    # learning rate 스케쥴러안? : https://wikidocs.net/157282\n","    # get_linear_schedule_with_warmup를 사용할 것! : https://huggingface.co/transformers/v3.0.2/main_classes/optimizer_schedules.html#transformers.get_linear_schedule_with_warmup\n","    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","    return model, optimizer, scheduler, epochs"],"metadata":{"id":"hovTSFWax1Rp","executionInfo":{"status":"ok","timestamp":1722251396526,"user_tz":-540,"elapsed":3,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"id":"hovTSFWax1Rp","execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["위에서 정의된 함수가 성공적으로 구현되어 실행되는지 확인해보자."],"metadata":{"id":"--HX2M9GzStb"},"id":"--HX2M9GzStb"},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","BertModelInitialization()\n","print(get_model_with_params(200000, device))"],"metadata":{"id":"ysnYWtl5x50h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722251409161,"user_tz":-540,"elapsed":12638,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"62155f12-1803-4e77-92fe-5d361952a9e5"},"id":"ysnYWtl5x50h","execution_count":22,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["(BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=2, bias=True)\n","), AdamW (\n","Parameter Group 0\n","    amsgrad: False\n","    betas: (0.9, 0.999)\n","    capturable: False\n","    differentiable: False\n","    eps: 1e-08\n","    foreach: None\n","    fused: None\n","    initial_lr: 1e-05\n","    lr: 1e-05\n","    maximize: False\n","    weight_decay: 0.01\n","), <torch.optim.lr_scheduler.LambdaLR object at 0x7970585c0e80>, 1)\n"]}]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"vWPXH_xRyjIh"},"id":"vWPXH_xRyjIh"},{"cell_type":"markdown","source":["모델의 예측 정확도를 산출하는 함수인 **accuracy**를 정의하자. 해당 함수는 학습한 모델의 validation 점수와 test의 결과를 계산할 때 사용된다.\n"],"metadata":{"id":"jI0maQgyyowE"},"id":"jI0maQgyyowE"},{"cell_type":"code","source":["# 정확도 계산 함수\n","def accuracy(preds, labels):\n","    f_pred = np.argmax(preds, axis=1).flatten()\n","    f_labels = labels.flatten()\n","    return np.sum(f_pred == f_labels) / len(f_labels)"],"metadata":{"id":"W41Kqvi-yj9e","executionInfo":{"status":"ok","timestamp":1722251409161,"user_tz":-540,"elapsed":15,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"id":"W41Kqvi-yj9e","execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["**잠깐 ✔ 랜덤시드 고정이란 무엇인가?**\n","> 학습된 모델의 결과를 동일하게 재현(Reproduction)하는 것은 여러가지 상황에서 팔요하다.  \n","> 모델을 돌릴 때마다 결과가 달라지지 않도록 고정하는 것이다.  \n","\n","- 수상자가 되어 코드의 정합성을 검증 받게 될 경우,\n","\n","- 경진대회 참가 도중 팀을 이루어 결과를 공유해야 되는 경우,\n","\n","- 논문을 작성하여 그 결과를 Reproduction 해야하는 경우 등 여러 상황에서 필요하다.  \n","\n","- 본 과제 역시, (1) preproc.ipynb 내 섹터 별 자동 점수 반환 및 (2) 최종 평가 과정에서 혼동을 방지하기 위하여 랜덤시드를 고정해야 한다. 주어진 2022 시드 값을 절대 수정하지 않도록 하자.\n","\n","참고 자료:\n","https://dacon.io/codeshare/2363\n","https://pytorch.org/docs/stable/notes/randomness.html\n"],"metadata":{"id":"48ZaUhLPyqPA"},"id":"48ZaUhLPyqPA"},{"cell_type":"code","source":["# 재현을 위해 랜덤시드 고정\n","seed_val = 2022"],"metadata":{"id":"vxHcsrw_ymfl","executionInfo":{"status":"ok","timestamp":1722251409161,"user_tz":-540,"elapsed":15,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"id":"vxHcsrw_ymfl","execution_count":24,"outputs":[]},{"cell_type":"code","source":["# 랜덤하게 데이터를 추출하기 위한 seed 값 설정\n","random.seed(seed_val)\n","np.random.seed(seed_val)\n","torch.manual_seed(seed_val)\n","torch.cuda.manual_seed_all(seed_val)"],"metadata":{"id":"r-_lCpk3zpsf","executionInfo":{"status":"ok","timestamp":1722251409161,"user_tz":-540,"elapsed":14,"user":{"displayName":"명수연","userId":"12733991230199599473"}}},"id":"r-_lCpk3zpsf","execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["학습에 활용될 데이터셋 및 토크나이저를 지정하자. 이후, 데이터셋을 전처리하여 train, validation, test 각각의 데이터로더에 입력하자."],"metadata":{"id":"uOYCrnJIzqJX"},"id":"uOYCrnJIzqJX"},{"cell_type":"code","source":["from tokenization import KoBertTokenizer\n","\n","# 전체 데이터를 불러오자.\n","whole_dataset = pd.read_csv('/content/gdrive/MyDrive/OUTTA/week12/ratings.txt', delimiter=\"\\t\")\n","\n","# KoBERTTokenizer를 불러오자.\n","tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n","\n","# 방금 전에 만든 preproc 함수 사용하기\n","train_dataloader, validation_dataloader, test_dataloader = preproc(tokenizer, whole_dataset)"],"metadata":{"id":"3SyFCdN9zryt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722251457421,"user_tz":-540,"elapsed":48274,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"b34936a8-d176-4f65-d672-b497c1d4dc18"},"id":"3SyFCdN9zryt","execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'KoBertTokenizer'.\n"]},{"output_type":"stream","name":"stdout","text":["length of whole_data : 200000\n","length of train_data : 180000\n"]}]},{"cell_type":"markdown","source":["BertModel을 생성하여 GPU 혹은 CPU에 등록하자.\n","- 이때 BertModelInitialization()를 실행할 경우 기존 Device에 등록된 BertModel은 초기화되니, 한 번만 실행한 이후로는 사용하지 않도록 유의하여 사용해야 한다.\n","- 디바이스를 설정하자.\n","- 본격적인 학습에 앞서 train에 대한 model, 옵티마이저, 스케줄러, 에폭을 지정하고, 모델의 그래디언트를 초기화하자."],"metadata":{"id":"X80OvtLZzuto"},"id":"X80OvtLZzuto"},{"cell_type":"code","source":["# 기존 Device에 등록된 BertModel은 초기화되니, 유의하여 사용할 것.\n","# 한 번만 실행하고, 그 이후로는 사용하지 않도록 조심!\n","BertModelInitialization()"],"metadata":{"id":"JqQtG27mztNU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722251460045,"user_tz":-540,"elapsed":2638,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"a7f85b98-aa4f-4523-9cee-20d8261c0794"},"id":"JqQtG27mztNU","execution_count":27,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# GPU 디바이스 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 방금 전에 만든 get_model_with_params 함수 사용하기\n","model, optimizer, scheduler, epochs = get_model_with_params(200000, device)\n","\n","# 그래디언트 초기화\n","model.zero_grad()"],"metadata":{"id":"8i3cPdiAzwpx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722251462978,"user_tz":-540,"elapsed":2935,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"0198ce52-ad37-4763-e049-6e42f23f025a"},"id":"8i3cPdiAzwpx","execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"markdown","source":["**train 및 validation**  \n","본격적으로 학습을 진행해보자. epoch 만큼 학습 loop를 반복할 것이다.\n","- 우리가 생성한 model에, 배치 데이터에 대한 input_ids, attention_mask, labels 변수를 입력하여 순전파를 진행할 것이다.\n","- 이후 역전파 과정을 통해 매개변수가 조절되며 학습이 이루어진다.\n","- 한 차례 학습이 이루어질 때마다 average training loss 및 validation 정확도를 출력할 것이다.\n","- 학습이 완료된 모델을 특정 경로(PATH)에 저장할 것이다.\n"],"metadata":{"id":"c83K5Z9Uzyu-"},"id":"c83K5Z9Uzyu-"},{"cell_type":"code","source":["# 에폭만큼 반복\n","for epoch_i in range(epochs):\n","    print(\"\")\n","    print('========{:}번째 Epoch / 전체 {:}회 ========'.format(epoch_i + 1, epochs))\n","    print('훈련 중')\n","\n","    total_loss = 0 # 로스 초기화\n","    sum_loss = 0\n","    model.train()  # 훈련모드로 변경\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for step, batch in enumerate(tqdm(train_dataloader)):\n","\n","        if step % 50 == 0:\n","          print(\"{}번째 까지의 평균 loss : {}\".format(step, sum_loss/50))\n","          sum_loss = 0\n","\n","        batch = tuple(t.to(device) for t in batch)   # 배치를 GPU에 넣음\n","        b_input_ids, b_input_mask, b_labels = batch  # 배치에서 데이터 추출\n","\n","        # 위에서 model 함수를 정의할 때 BertForSequenceClassification를 활용하였다.\n","        # 여기서 BertForSequenceClassification는 input_ids, attention_mask, labels 변수를 입력받는 'forward' 함수를 내장한다.\n","        # forward 함수는 forward(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states)와 같은 함수 파라미터를 갖는다.\n","        # model 함수를 통해 forward를 수행하기 위해 우리가 입력해야 하는 변수는 input_ids, attention_mask, labels 이다.\n","        # 위의 코드에서 정의한 배치 데이터를 model 함수에 입력하여, 배치에 대한 Forward를 수행해보자.\n","\n","        # 우리는 BertForSequenceClassification 함수를 모델로 사용하기로 하였다. 이 함수의 공식 문서를 잘 참고할것\n","        # 참고 링크 : https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#transformers.BertForSequenceClassification\n","        ##여기에 코드 작성\n","        outputs = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n","\n","        loss = outputs[0]\n","         # BertForSequenceClassification가 return하는 것이 어떤 것인지 꼭 확인해보자.\n","        total_loss += loss.item() # 총 로스 계산\n","        sum_loss += loss.item()\n","        loss.backward() # Backward 수행으로 그래디언트 계산\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # 그래디언트 클리핑\n","\n","        optimizer.step() # 옵티마이저의 step을 이용해서 그래디언트를 통해 가중치 파라미터 업데이트\n","        scheduler.step() # 스케줄러로 학습률 감소. https://wikidocs.net/157282 : 이 문서에서 learning rate scheduler의 사용법 잘 숙지하기\n","        model.zero_grad() # 그래디언트 초기화\n","\n","    # 평균 로스 계산\n","    avg_train_loss = total_loss / len(train_dataloader)\n","    print(\"\")\n","    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","\n","    #### 검증 ####\n","\n","    print(\"\")\n","    print(\"검증 중\")\n","\n","    model.eval()\n","\n","    # 변수 초기화\n","    eval_accuracy = 0\n","    nb_eval_steps = 0\n","\n","    # 데이터로더에서 배치만큼 반복하여 가져옴\n","    for batch in validation_dataloader:\n","        # 배치를 GPU에 넣음\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        # 배치에서 데이터 추출\n","        b_input_ids, b_input_mask, b_labels = batch\n","\n","        # 그래디언트 계산 안함\n","        with torch.no_grad():\n","            # Forward 수행\n","            outputs = model(b_input_ids, attention_mask=b_input_mask)\n","\n","        # 결과 값 구함. 이 때 validation에는 label을 입력하면 안된다.\n","        # validation은 label 값을 입력해서 학습을 하는 것이 아니라, input을 넣어서 예측값을 얻은 후에 그것의 accuracy를 구해야 하기 때문에 label값, 즉 정답값을 알려주면 안된다\n","        # https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#transformers.BertForSequenceClassification 이 링크를 참고하여, return 부분을 꼼꼼히 읽을 것.\n","        # training 할 때와 index가 다르다. 공식 문서를 꼼꼼하게 봐야 한다. 여기서 많이 실수한다.\n","        logits = outputs.logits\n","\n","        # CPU로 데이터 이동\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","\n","        # 출력 로짓과 라벨을 비교하여 정확도 계산\n","        tmp_eval_accuracy = accuracy(logits, label_ids)\n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","\n","    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n","\n","# 학습된 모델을 해당 PATH에 저장\n","PATH = \"/content/gdrive/MyDrive/model.pt\"\n","torch.save(model.state_dict(), PATH)\n","\n","print(\"\")\n","print(\"Training complete!\")"],"metadata":{"id":"kICVYZpvz0R5","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["879cfb3b2a974bb5966c183c698118ac","e4b1148e766b4d5dbe07bee9b61b5dd9","e629b1f77fd34d77922d5babbd452cbf","7d16f9f7802141b58ed673800a550feb","6ff68931a54e466d9098a0dfb56a766a","bfe8029835e7456ea9bfa11f283f14ad","2819636f6057461aa4e16d989f74000c","fc7435f78dbe421b9eefc0bcc319aa9f","ae2f4f2a8a674c9bbe865ea384ce8032","fd284581596a4b1287c31d69faeff322","d57686697063490b94c2b971d5831d76"]},"executionInfo":{"status":"ok","timestamp":1722254605676,"user_tz":-540,"elapsed":3142704,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"0fa9e8fd-ec63-4355-aea1-295c31314c7e"},"id":"kICVYZpvz0R5","execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","========1번째 Epoch / 전체 1회 ========\n","훈련 중\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/5063 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"879cfb3b2a974bb5966c183c698118ac"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["0번째 까지의 평균 loss : 0.0\n","50번째 까지의 평균 loss : 0.6856669664382935\n","100번째 까지의 평균 loss : 0.588418562412262\n","150번째 까지의 평균 loss : 0.524882071018219\n","200번째 까지의 평균 loss : 0.4720460492372513\n","250번째 까지의 평균 loss : 0.43258804827928543\n","300번째 까지의 평균 loss : 0.42900741487741473\n","350번째 까지의 평균 loss : 0.40108666628599166\n","400번째 까지의 평균 loss : 0.3978926604986191\n","450번째 까지의 평균 loss : 0.39440902799367905\n","500번째 까지의 평균 loss : 0.3892801660299301\n","550번째 까지의 평균 loss : 0.37210349619388583\n","600번째 까지의 평균 loss : 0.3811823514103889\n","650번째 까지의 평균 loss : 0.3652547678351402\n","700번째 까지의 평균 loss : 0.360320183634758\n","750번째 까지의 평균 loss : 0.34008264541625977\n","800번째 까지의 평균 loss : 0.38811068803071974\n","850번째 까지의 평균 loss : 0.37020388185977937\n","900번째 까지의 평균 loss : 0.35156447768211363\n","950번째 까지의 평균 loss : 0.3329201012849808\n","1000번째 까지의 평균 loss : 0.3382401105761528\n","1050번째 까지의 평균 loss : 0.3499823886156082\n","1100번째 까지의 평균 loss : 0.34665166959166527\n","1150번째 까지의 평균 loss : 0.33317376613616945\n","1200번째 까지의 평균 loss : 0.30950091749429703\n","1250번째 까지의 평균 loss : 0.3512546718120575\n","1300번째 까지의 평균 loss : 0.3424940660595894\n","1350번째 까지의 평균 loss : 0.33503748089075086\n","1400번째 까지의 평균 loss : 0.3243563209474087\n","1450번째 까지의 평균 loss : 0.3431740924715996\n","1500번째 까지의 평균 loss : 0.33629965394735334\n","1550번째 까지의 평균 loss : 0.32956039667129516\n","1600번째 까지의 평균 loss : 0.30432349637150763\n","1650번째 까지의 평균 loss : 0.3010034069418907\n","1700번째 까지의 평균 loss : 0.3439272019267082\n","1750번째 까지의 평균 loss : 0.3274938836693764\n","1800번째 까지의 평균 loss : 0.3131162559986114\n","1850번째 까지의 평균 loss : 0.31212936878204345\n","1900번째 까지의 평균 loss : 0.2857391607761383\n","1950번째 까지의 평균 loss : 0.31058505475521087\n","2000번째 까지의 평균 loss : 0.3119472999870777\n","2050번째 까지의 평균 loss : 0.30898316830396655\n","2100번째 까지의 평균 loss : 0.29961470782756805\n","2150번째 까지의 평균 loss : 0.28522166430950163\n","2200번째 까지의 평균 loss : 0.30525791585445405\n","2250번째 까지의 평균 loss : 0.2957022076845169\n","2300번째 까지의 평균 loss : 0.3142105257511139\n","2350번째 까지의 평균 loss : 0.3049999041855335\n","2400번째 까지의 평균 loss : 0.2850902369618416\n","2450번째 까지의 평균 loss : 0.3027054440975189\n","2500번째 까지의 평균 loss : 0.2869923198223114\n","2550번째 까지의 평균 loss : 0.3127077604830265\n","2600번째 까지의 평균 loss : 0.3017836062610149\n","2650번째 까지의 평균 loss : 0.290747642070055\n","2700번째 까지의 평균 loss : 0.31697807714343074\n","2750번째 까지의 평균 loss : 0.30125999957323074\n","2800번째 까지의 평균 loss : 0.2995464473962784\n","2850번째 까지의 평균 loss : 0.2794363060593605\n","2900번째 까지의 평균 loss : 0.2837562003731728\n","2950번째 까지의 평균 loss : 0.3011622253060341\n","3000번째 까지의 평균 loss : 0.3029595074057579\n","3050번째 까지의 평균 loss : 0.30759856693446636\n","3100번째 까지의 평균 loss : 0.2863595922291279\n","3150번째 까지의 평균 loss : 0.2759084178507328\n","3200번째 까지의 평균 loss : 0.32152525007724764\n","3250번째 까지의 평균 loss : 0.30587625205516816\n","3300번째 까지의 평균 loss : 0.2723721280694008\n","3350번째 까지의 평균 loss : 0.2645434708893299\n","3400번째 까지의 평균 loss : 0.27662389680743216\n","3450번째 까지의 평균 loss : 0.27025592744350435\n","3500번째 까지의 평균 loss : 0.2765118570625782\n","3550번째 까지의 평균 loss : 0.28179285421967504\n","3600번째 까지의 평균 loss : 0.2975015975534916\n","3650번째 까지의 평균 loss : 0.2825776481628418\n","3700번째 까지의 평균 loss : 0.29608393147587775\n","3750번째 까지의 평균 loss : 0.29047483801841734\n","3800번째 까지의 평균 loss : 0.265237318277359\n","3850번째 까지의 평균 loss : 0.30457789957523346\n","3900번째 까지의 평균 loss : 0.28574232906103136\n","3950번째 까지의 평균 loss : 0.2824542714655399\n","4000번째 까지의 평균 loss : 0.2737692728638649\n","4050번째 까지의 평균 loss : 0.2660195803642273\n","4100번째 까지의 평균 loss : 0.25962259963154793\n","4150번째 까지의 평균 loss : 0.24478042587637902\n","4200번째 까지의 평균 loss : 0.25789486303925513\n","4250번째 까지의 평균 loss : 0.258494620770216\n","4300번째 까지의 평균 loss : 0.3035741217434406\n","4350번째 까지의 평균 loss : 0.2673721498250961\n","4400번째 까지의 평균 loss : 0.3031777074933052\n","4450번째 까지의 평균 loss : 0.2917062985897064\n","4500번째 까지의 평균 loss : 0.31040340423583984\n","4550번째 까지의 평균 loss : 0.293641432672739\n","4600번째 까지의 평균 loss : 0.27160531520843506\n","4650번째 까지의 평균 loss : 0.28700431033968926\n","4700번째 까지의 평균 loss : 0.28937857419252394\n","4750번째 까지의 평균 loss : 0.2588415086269379\n","4800번째 까지의 평균 loss : 0.3024133852124214\n","4850번째 까지의 평균 loss : 0.27591618061065676\n","4900번째 까지의 평균 loss : 0.25938933685421944\n","4950번째 까지의 평균 loss : 0.2648528205603361\n","5000번째 까지의 평균 loss : 0.28540705621242524\n","5050번째 까지의 평균 loss : 0.2892493796348572\n","\n","  Average training loss: 0.32\n","\n","검증 중\n","  Accuracy: 0.89\n","\n","Training complete!\n"]}]},{"cell_type":"markdown","source":["**test**  \n","학습된 모델에 test용 데이터를 입력하여 test 결과를 출력하자.\n","- test용 데이터로더를 활용하여 배치 데이터에 대한 input_ids, mask를 model에 입력하여 순전파를 실행하고, 실행 결과를 outputs에 저장하자.\n","- 이때 test는 '학습'이 목적이 아니므로 그래디언트를 계산하지 않도록 한다.\n","- outputs와 실제 정답인 label_ids를 비교하여 모델의 최종 test accuracy를 확인하자. 모델이 88%의 정확도를 넘기는가?\n"],"metadata":{"id":"8w-3_4Q1z24E"},"id":"8w-3_4Q1z24E"},{"cell_type":"code","source":["print(\"\")\n","print(\"테스트 중\")\n","\n","model.eval()\n","\n","# 변수 초기화\n","eval_accuracy = 0\n","nb_eval_steps = 0\n","\n","# 데이터로더에서 배치만큼 반복하여 가져옴\n","for batch in test_dataloader:\n","  # 배치를 GPU에 넣음\n","  batch = tuple(t.to(device) for t in batch)\n","\n","  # 배치에서 데이터 추출\n","  b_input_ids, b_input_mask, b_labels = batch\n","\n","  # 그래디언트 계산 안함\n","  with torch.no_grad():\n","      # Forward 수행\n","      outputs = model(b_input_ids, attention_mask=b_input_mask)\n","  # 결과 값 구함\n","  # Validation 때 했던 것과 같다.\n","  # https://huggingface.co/transformers/v3.0.2/model_doc/bert.html#transformers.BertForSequenceClassification 이 링크를 참고하여, return 부분을 꼼꼼히 읽을 것.\n","  # training 할 때와 index가 다르다. 공식 문서를 꼼꼼하게 봐야 한다. 여기서 많이 실수한다.\n","  logits = outputs.logits\n","\n","  # CPU로 데이터 이동\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","\n","  # 출력 로짓과 라벨을 비교하여 정확도 계산\n","  tmp_eval_accuracy = accuracy(logits, label_ids)\n","  eval_accuracy += tmp_eval_accuracy\n","  nb_eval_steps += 1\n","\n","print(\"\")\n","print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))"],"metadata":{"id":"IdVH4_AOz4Ey","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722254733961,"user_tz":-540,"elapsed":128288,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"d2f108e2-b2e7-4dea-bf11-57a80e4d495e"},"id":"IdVH4_AOz4Ey","execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","테스트 중\n","\n","  Accuracy: 0.89\n"]}]},{"cell_type":"markdown","source":["# main"],"metadata":{"id":"qNyV1HB9z6XM"},"id":"qNyV1HB9z6XM"},{"cell_type":"markdown","source":["직접 우리가 텍스트를 입력해보고 결과를 관찰해보자"],"metadata":{"id":"0kBh3rh-0F0c"},"id":"0kBh3rh-0F0c"},{"cell_type":"code","source":["# GPU 디바이스 설정\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model = get_model(device)"],"metadata":{"id":"mBRKxlyHz7WW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722254736111,"user_tz":-540,"elapsed":2165,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"f4f95f97-e2dc-4439-e265-bbc4cbf54981"},"id":"mBRKxlyHz7WW","execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/kobert and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["while True:\n","  input_str = input()\n","\n","  # q를 입력하면 while 루프 끝\n","  if input_str == \"q\":\n","    break\n","\n","  # 입력받은 문장을 토큰화\n","  tokenizer = KoBertTokenizer.from_pretrained(\"monologg/kobert\")\n","  tokenized_data = [data_to_token_ids(tokenizer, input_str)]\n","  attention_masks = [token_ids_to_mask(token_ids) for token_ids in tokenized_data]\n","\n","  # 토큰을 텐서로 변환\n","  inputs_tensor = torch.tensor(tokenized_data).to(device)\n","  masks_tensor = torch.tensor(attention_masks).to(device)\n","\n","  # 신경망의 Forward 함수 활용\n","  outputs = model(inputs_tensor, attention_mask=masks_tensor)\n","\n","  # 확률 값에 따른 문장 분류\n","  logits = outputs[0]\n","\n","  # 조건문을 활용하여 감정 이진 분류 결과를 출력하라.\n","  # 각 라벨이 어떤 말을 의미하는지 생각해보자.\n","  # 만약 \"나는 당신을 정말로 사랑합니다\" 라는 말을 입력하면 \"당신이 입력한 문장은 긍정입니다.\"라는 문구가 출력되어야 하고\n","  # \"화가 너무 난다. 진짜 짜증나네\" 라는 말을 입력하면 \"당신이 입력한 문장은 부정입니다.\"라는 문구가 출력되어야 합니다.\n","  ## 여기에 코드 작성. if-else 문을 사용해야함\n","  if logits[0][0] > logits[0][1]:\n","    print(\"당신이 입력한 문장은 부정입니다.\")\n","  else:\n","    print(\"당신이 입력한 문장은 긍정입니다.\")\n","\n"],"metadata":{"id":"k_Xapkrq0DIO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1722254811930,"user_tz":-540,"elapsed":15222,"user":{"displayName":"명수연","userId":"12733991230199599473"}},"outputId":"ad324981-64d0-4637-bdcb-09d2b80ed58a"},"id":"k_Xapkrq0DIO","execution_count":34,"outputs":[{"name":"stdout","output_type":"stream","text":["나는 당신을 정말로 사랑합니다\n"]},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'KoBertTokenizer'.\n"]},{"name":"stdout","output_type":"stream","text":["당신이 입력한 문장은 긍정입니다.\n","화가 너무 난다. 진짜 짜증나네\n"]},{"output_type":"stream","name":"stderr","text":["The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n","The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n","The class this function is called from is 'KoBertTokenizer'.\n"]},{"name":"stdout","output_type":"stream","text":["당신이 입력한 문장은 부정입니다.\n","q\n"]}]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"291cb03fe68e434db50c38fdd02b4502":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_79d8e815bf7b473b8be7513ce9d05d5f","IPY_MODEL_8c1437d2743046398113a9c528c69b8f","IPY_MODEL_f608397c0de14b7aa0995e62b4526b0f"],"layout":"IPY_MODEL_2c3fc077c0074895b118ec38d2874c22"}},"79d8e815bf7b473b8be7513ce9d05d5f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_779d20f9061441cf98b59684043fdaf1","placeholder":"​","style":"IPY_MODEL_b89b7b825c88471085e6630c04c93bac","value":" 24%"}},"8c1437d2743046398113a9c528c69b8f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_1fe3685e8f7a4d5fa83f8f1b4c6685f8","max":5063,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3bf4fcf4f0884df7990f09f38c8b8d13","value":1235}},"f608397c0de14b7aa0995e62b4526b0f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_303ed69f45534fa385109d30b6797334","placeholder":"​","style":"IPY_MODEL_c41b2fd5e14e4356ab4217c722d6aabc","value":" 1235/5063 [00:00&lt;00:01, 2718.12it/s]"}},"2c3fc077c0074895b118ec38d2874c22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"779d20f9061441cf98b59684043fdaf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b89b7b825c88471085e6630c04c93bac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1fe3685e8f7a4d5fa83f8f1b4c6685f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3bf4fcf4f0884df7990f09f38c8b8d13":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"303ed69f45534fa385109d30b6797334":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c41b2fd5e14e4356ab4217c722d6aabc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"879cfb3b2a974bb5966c183c698118ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e4b1148e766b4d5dbe07bee9b61b5dd9","IPY_MODEL_e629b1f77fd34d77922d5babbd452cbf","IPY_MODEL_7d16f9f7802141b58ed673800a550feb"],"layout":"IPY_MODEL_6ff68931a54e466d9098a0dfb56a766a"}},"e4b1148e766b4d5dbe07bee9b61b5dd9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bfe8029835e7456ea9bfa11f283f14ad","placeholder":"​","style":"IPY_MODEL_2819636f6057461aa4e16d989f74000c","value":"100%"}},"e629b1f77fd34d77922d5babbd452cbf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fc7435f78dbe421b9eefc0bcc319aa9f","max":5063,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ae2f4f2a8a674c9bbe865ea384ce8032","value":5063}},"7d16f9f7802141b58ed673800a550feb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd284581596a4b1287c31d69faeff322","placeholder":"​","style":"IPY_MODEL_d57686697063490b94c2b971d5831d76","value":" 5063/5063 [50:21&lt;00:00,  1.76it/s]"}},"6ff68931a54e466d9098a0dfb56a766a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfe8029835e7456ea9bfa11f283f14ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2819636f6057461aa4e16d989f74000c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fc7435f78dbe421b9eefc0bcc319aa9f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ae2f4f2a8a674c9bbe865ea384ce8032":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd284581596a4b1287c31d69faeff322":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d57686697063490b94c2b971d5831d76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}